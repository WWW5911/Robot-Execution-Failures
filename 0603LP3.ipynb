{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd063fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d",
   "display_name": "Python 3.9.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import math\n",
    "import time \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "np.random.seed(22995386)\n",
    "\n",
    "# Data loading\n",
    "df = pd.read_csv('dataset\\\\LP3\\\\LP3_DP.csv')\n",
    "\n",
    "ylabels = df.Class.unique()\n",
    "output_dim = len(ylabels)\n",
    "full_data = df.to_numpy()\n",
    "full_data = np.transpose(np.transpose(full_data)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "lp_tmp = []\n",
    "np.random.shuffle(full_data)\n",
    "for tp in full_data:\n",
    "    train_data.append(tp[1:])\n",
    "    lp_tmp.append(tp[0])\n",
    "\n",
    "# oneHot Encoding\n",
    "train_label = np.zeros( ( len(train_data), output_dim ) )\n",
    "for i in range(len(lp_tmp)):\n",
    "    train_label[i][lp_tmp[i]-1] = 1     # let 1-5 turns to 0-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "x_scale = preprocessing.scale(train_data)\n",
    "x_normalized = ( preprocessing.normalize(x_scale, norm='l2')  ) \n",
    "# if x_normalized.min() < 0:\n",
    "#     x_normalized = x_normalized - x_normalized.min()\n",
    "#     x_normalized = x_normalized * 255 / x_normalized.max()\n",
    "pca=PCA(whiten=True)\n",
    "# newData=pca.fit_transform(x_normalized)\n",
    "newData = x_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Minmax_scale(w):\n",
    "    return np.array( (w-w.min()) / (w.max()-w.min()) )\n",
    "minmax = 0\n",
    "def Partition(data, label, ratio):\n",
    "    if minmax == 1 :\n",
    "        data = np.transpose(data).astype(np.float64)\n",
    "        for i in range (len(data) ):\n",
    "            data[i] = Minmax_scale(data[i])\n",
    "        data = np.transpose(data)\n",
    "    t_data = data[0:int(len(data)*ratio) ]\n",
    "    t_label = label[0:int(len(label)*ratio) ]\n",
    "    verify_data = data[int(len(data)*ratio):len(data)]\n",
    "    verify_label = label[int(len(data)*ratio):len(data)]\n",
    "    return t_data, t_label, verify_data, verify_label\n",
    "\n",
    "def Prediction(a):\n",
    "    index = 0\n",
    "    for i in range( len(a) ):\n",
    "        if a[i] > a[index]:\n",
    "            index = i\n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "train_ratio = 0.8\n",
    "t_data, t_label, verify_data, verify_label = Partition(newData, train_label, train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(37, 144)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "#cnn CONV\n",
    "np.random.seed(225)\n",
    "def create_filter(in_ch, out_ch, keneral):\n",
    "    return np.random.randint(3, size=(out_ch, keneral[0], keneral[1] )) -1\n",
    "\n",
    "def conv(data, l1_filter , stride, padding ):\n",
    "    if padding > 0:\n",
    "        for i in range(padding):\n",
    "            data = np.insert(data, 0, np.zeros( len(data) ), axis=1 )\n",
    "            data = np.insert(data, len(data[0]), np.zeros( len(data) ), axis=1 )\n",
    "            data = np.insert(data, 0, np.zeros( len(data[0]) ), axis=0 )\n",
    "            data = np.insert(data, len(data), np.zeros( len(data[0]) ), axis=0 )\n",
    "    result = []\n",
    "    keneral = [ len(l1_filter[0]), len(l1_filter[0][0]) ]\n",
    "    for filt in l1_filter:\n",
    "        i = 0\n",
    "        tmp = []\n",
    "        while(i <= len(data)- keneral[0]):\n",
    "            j = 0\n",
    "            while(j <= len(data[i])- keneral[1] ):\n",
    "                tmp.append( sum(sum( data[i : i+keneral[0] ,j : j+keneral[1] ] * filt) )    )\n",
    "                j += stride\n",
    "            i += stride\n",
    "        tmp = np.reshape(tmp, (len(data)- keneral[0]+1, len(data[i])- keneral[1]+1)  )\n",
    "        result.append(tmp)\n",
    "    return result\n",
    "\n",
    "def pooling(data, fuct, size):\n",
    "    result = []\n",
    "    for layer in data:\n",
    "        i = 0\n",
    "        tmp = []\n",
    "        while(i < len(layer) ):\n",
    "            j = 0\n",
    "            while(j < len(layer[i]) ):\n",
    "                tmp.append( fuct([fuct( layer[k][j:j+size] )for k in range(i,i+size)] )    )\n",
    "                j += size\n",
    "            i += size\n",
    "        tmp = np.reshape(tmp, ( int(len(layer)/size), int(len(layer[0])/size) )  )\n",
    "        result.append(tmp)\n",
    "    return result\n",
    "\n",
    "l1_filter = create_filter( 1, 16, [3,4])\n",
    "cnn_t_data = [  np.ndarray.flatten( np.asarray( pooling( conv(  np.reshape(np.asarray(t_data[i]), (9,10)), l1_filter , 1, 1), max, 3 ) ) ) for i in range(len(t_data) )]\n",
    "cnn_verify_data = [  np.ndarray.flatten( np.asarray( pooling( conv(  np.reshape(np.asarray(verify_data[i]), (9,10)), l1_filter , 1, 1), max, 3 ) ) ) for i in range(len(verify_data) )]\n",
    "l2_filter = create_filter( 1, 16, [3,3])\n",
    "cnn_t_data = [  np.ndarray.flatten( np.asarray( pooling( conv(  np.reshape(np.asarray(cnn_t_data[i]), (12,12)), l2_filter , 1, 1), max, 4 ) ) ) for i in range(len(cnn_t_data) )]\n",
    "cnn_verify_data = [  np.ndarray.flatten( np.asarray( pooling( conv(  np.reshape(np.asarray(cnn_verify_data[i]), (12,12)), l2_filter , 1, 1), max, 4 ) ) ) for i in range(len(cnn_verify_data) )]\n",
    "np.shape(cnn_t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "12.0, 17.0, 7.0, 1.0\n3.0, 3.0, 2.0, 2.0\n"
     ]
    }
   ],
   "source": [
    "print(str(sum(t_label.T[0])) + \", \" + str(sum(t_label.T[1])) + \", \" + str(sum(t_label.T[2]))+ \", \" + str(sum(t_label.T[3])) )\n",
    "print(str(sum(verify_label.T[0])) + \", \" + str(sum(verify_label.T[1])) + \", \" + str(sum(verify_label.T[2]))+ \", \" + str(sum(verify_label.T[3])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:0\n",
      "0.020516088260970386\n",
      "training acc: 0.4594594594594595\n",
      "testing acc: 0.3\n",
      "Epoch:0\n",
      "0.020516088260970386\n",
      "acc: 0.4594594594594595\n",
      "testing acc: 0.3\n",
      "Epoch:1\n",
      "0.020349296352739663\n",
      "training acc: 0.4594594594594595\n",
      "testing acc: 0.3\n",
      "Epoch:2\n",
      "0.020109889796173775\n",
      "training acc: 0.4594594594594595\n",
      "testing acc: 0.3\n",
      "Epoch:3\n",
      "0.019790283938321516\n",
      "training acc: 0.4594594594594595\n",
      "testing acc: 0.3\n",
      "Epoch:4\n",
      "0.01940904520152693\n",
      "training acc: 0.4594594594594595\n",
      "testing acc: 0.3\n",
      "Epoch:5\n",
      "0.019067596167957286\n",
      "training acc: 0.4594594594594595\n",
      "testing acc: 0.3\n",
      "Epoch:6\n",
      "0.018870244492484846\n",
      "training acc: 0.4594594594594595\n",
      "testing acc: 0.3\n",
      "Epoch:7\n",
      "0.01877370769521283\n",
      "training acc: 0.4594594594594595\n",
      "testing acc: 0.3\n",
      "Epoch:8\n",
      "0.018684640287308034\n",
      "training acc: 0.4594594594594595\n",
      "testing acc: 0.3\n",
      "Epoch:9\n",
      "0.018582973250992722\n",
      "training acc: 0.4594594594594595\n",
      "testing acc: 0.3\n",
      "Epoch:10\n",
      "0.018475283195800028\n",
      "training acc: 0.5135135135135135\n",
      "testing acc: 0.3\n",
      "Epoch:11\n",
      "0.018357891831587838\n",
      "training acc: 0.7297297297297297\n",
      "testing acc: 0.3\n",
      "Epoch:12\n",
      "0.01823237214517733\n",
      "training acc: 0.7837837837837838\n",
      "testing acc: 0.3\n",
      "Epoch:13\n",
      "0.01808270252560768\n",
      "training acc: 0.7837837837837838\n",
      "testing acc: 0.4\n",
      "Epoch:14\n",
      "0.017898929095163756\n",
      "training acc: 0.7837837837837838\n",
      "testing acc: 0.5\n",
      "Epoch:15\n",
      "0.01768493049331091\n",
      "training acc: 0.7837837837837838\n",
      "testing acc: 0.6\n",
      "Epoch:16\n",
      "0.01748450404105176\n",
      "training acc: 0.8108108108108109\n",
      "testing acc: 0.6\n",
      "Epoch:17\n",
      "0.01731893345659192\n",
      "training acc: 0.8108108108108109\n",
      "testing acc: 0.6\n",
      "Epoch:18\n",
      "0.01718884545947793\n",
      "training acc: 0.8108108108108109\n",
      "testing acc: 0.6\n",
      "Epoch:19\n",
      "0.017069918437540486\n",
      "training acc: 0.8648648648648649\n",
      "testing acc: 0.6\n",
      "Epoch:20\n",
      "0.016957771699736814\n",
      "training acc: 0.8918918918918919\n",
      "testing acc: 0.6\n",
      "Epoch:21\n",
      "0.016842272919055056\n",
      "training acc: 0.918918918918919\n",
      "testing acc: 0.6\n",
      "Epoch:22\n",
      "0.016734717028736634\n",
      "training acc: 0.918918918918919\n",
      "testing acc: 0.6\n",
      "Epoch:23\n",
      "0.016642236313391463\n",
      "training acc: 0.918918918918919\n",
      "testing acc: 0.6\n",
      "Epoch:24\n",
      "0.016570632016031824\n",
      "training acc: 0.918918918918919\n",
      "testing acc: 0.5\n",
      "Epoch:25\n",
      "0.016509739394994265\n",
      "training acc: 0.9459459459459459\n",
      "testing acc: 0.6\n",
      "Epoch:26\n",
      "0.01644745808151885\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.6\n",
      "Epoch:27\n",
      "0.016399481741288354\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.6\n",
      "Epoch:28\n",
      "0.016365717125034752\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:29\n",
      "0.016348141597255988\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:30\n",
      "0.016340764444879235\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:31\n",
      "0.016337819593374183\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:32\n",
      "0.01633622736239451\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:51\n",
      "0.01633561671411495\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:52\n",
      "0.01633470795009152\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:53\n",
      "0.016333702421040323\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:54\n",
      "0.0163325916749879\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:55\n",
      "0.016331369132130055\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:56\n",
      "0.016330024022051155\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:57\n",
      "0.016328579670674092\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:58\n",
      "0.016326932172604596\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:59\n",
      "0.016325148702799795\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:60\n",
      "0.01632315449966315\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:61\n",
      "0.016317687841598965\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:62\n",
      "0.01631589888046047\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:63\n",
      "0.01631299314858532\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:64\n",
      "0.016306324900720485\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:65\n",
      "0.0163032163541364\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:66\n",
      "0.016294936767192195\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:67\n",
      "0.016284131417098578\n",
      "training acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:68\n",
      "0.01627435778465647\n",
      "training acc: 1.0\n",
      "testing acc: 0.5\n",
      "Epoch:69\n",
      "0.01626124825135385\n",
      "training acc: 1.0\n",
      "testing acc: 0.5\n",
      "Epoch:70\n",
      "0.016245331079137858\n",
      "training acc: 1.0\n",
      "testing acc: 0.5\n",
      "Epoch:71\n",
      "0.01622950545817592\n",
      "training acc: 1.0\n",
      "testing acc: 0.5\n",
      "Epoch:72\n",
      "0.01621463103525999\n",
      "training acc: 1.0\n",
      "testing acc: 0.5\n",
      "Epoch:73\n",
      "0.016199954545262785\n",
      "training acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:74\n",
      "0.016187667618100882\n",
      "training acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:75\n",
      "0.016179197156969173\n",
      "training acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:76\n",
      "0.01617456059916512\n",
      "training acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:77\n",
      "0.01617096528312949\n",
      "training acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:78\n",
      "0.01616893587885676\n",
      "training acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:79\n",
      "0.01616807053392172\n",
      "training acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:80\n",
      "0.016167656060681018\n",
      "training acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:81\n",
      "0.016167438758429686\n",
      "training acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:82\n",
      "0.016167325900365525\n",
      "training acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:83\n",
      "0.016167260559481197\n",
      "training acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:84\n",
      "0.01616721095788035\n",
      "training acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:85\n",
      "0.016167165073406268\n",
      "training acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:86\n",
      "0.01616712489251763\n",
      "training acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:87\n",
      "0.016167096488879838\n",
      "training acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:88\n",
      "0.016167081005386927\n",
      "training acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:89\n",
      "0.01616707539432148\n",
      "training acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:100\n",
      "0.016167122911501314\n",
      "acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:200\n",
      "0.01616904120470504\n",
      "acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:300\n",
      "0.016897705657757313\n",
      "acc: 0.8918918918918919\n",
      "testing acc: 0.4\n",
      "Epoch:400\n",
      "0.016533912534344535\n",
      "acc: 0.9459459459459459\n",
      "testing acc: 0.4\n",
      "Epoch:500\n",
      "0.016534351302246876\n",
      "acc: 0.9459459459459459\n",
      "testing acc: 0.4\n",
      "Epoch:600\n",
      "0.016341031239884365\n",
      "acc: 0.972972972972973\n",
      "testing acc: 0.4\n",
      "Epoch:700\n",
      "0.01634101973475114\n",
      "acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:800\n",
      "0.016168237325699334\n",
      "acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:900\n",
      "0.016168906664308445\n",
      "acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:1000\n",
      "0.01616893306516051\n",
      "acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:1100\n",
      "0.01616894010756467\n",
      "acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:1200\n",
      "0.016168807531857333\n",
      "acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:1300\n",
      "0.016168831070031975\n",
      "acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:1400\n",
      "0.017112634929102474\n",
      "acc: 0.8648648648648649\n",
      "testing acc: 0.4\n",
      "Epoch:1500\n",
      "0.01662028559134597\n",
      "acc: 0.918918918918919\n",
      "testing acc: 0.3\n",
      "Epoch:1600\n",
      "0.016526209586419007\n",
      "acc: 0.9459459459459459\n",
      "testing acc: 0.3\n",
      "Epoch:1700\n",
      "0.01636003270211927\n",
      "acc: 0.972972972972973\n",
      "testing acc: 0.5\n",
      "Epoch:1800\n",
      "0.016346427773236357\n",
      "acc: 0.972972972972973\n",
      "testing acc: 0.4\n",
      "Epoch:1900\n",
      "0.016168521683176038\n",
      "acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:2000\n",
      "0.016169034037126773\n",
      "acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:2100\n",
      "0.016168981512982624\n",
      "acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:2200\n",
      "0.016168955324382308\n",
      "acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:2300\n",
      "0.016168926773801\n",
      "acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:2400\n",
      "0.016168953027709543\n",
      "acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:2500\n",
      "0.016168974862427847\n",
      "acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:2600\n",
      "0.01616893770748721\n",
      "acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:2700\n",
      "0.016168786829148355\n",
      "acc: 1.0\n",
      "testing acc: 0.6\n",
      "Epoch:2750\n",
      "0.01755124419775211\n",
      "training acc: 0.8108108108108109\n",
      "testing acc: 0.8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(35948766)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input, output):\n",
    "        super(MLP,self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input, 30),\n",
    "            nn.BatchNorm1d(30),\n",
    "            nn.ReLU6()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(30, 15),\n",
    "            nn.ReLU6()\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(15, 30),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Linear(30, 15),\n",
    "            \n",
    "        )\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Linear(15, output),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        return x\n",
    "model = MLP(len(cnn_t_data[0]), output_dim)\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0095,weight_decay=0.0001 )\n",
    "Epoch = 2760\n",
    "\n",
    "maxt = 0\n",
    "maxv = 0\n",
    "minl = 10000\n",
    "for epoch in range(Epoch):\n",
    "    acc_t = 0\n",
    "    acc_v = 0\n",
    "    mean_loss = 0\n",
    "    tmp1, tmp2, tmp3 = maxt, maxv, minl\n",
    "    optimizer.zero_grad()\n",
    "    inputs = torch.autograd.Variable( torch.FloatTensor(cnn_t_data))\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion( outputs, torch.tensor(t_label) )\n",
    "    mean_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# if there exist better loss or training accuracy or testing accuracy \n",
    "    minl = min(minl, mean_loss/len(t_data))\n",
    "    for i in range (len(outputs)):\n",
    "        p = Prediction(outputs[i])\n",
    "        if t_label[i][p] == 1:\n",
    "            acc_t += 1\n",
    "    var_in = torch.autograd.Variable( torch.FloatTensor(cnn_verify_data))\n",
    "    var_out = model(var_in)\n",
    "    for i in range (len(var_out)):\n",
    "        p = Prediction(var_out[i])\n",
    "        if verify_label[i][p] == 1:\n",
    "            acc_v += 1\n",
    "    maxt = max(maxt, acc_t / len(t_data))\n",
    "    maxv = max(maxv, acc_v / len(verify_data))\n",
    "    if tmp1 != maxt or tmp2 != maxv or tmp3 != minl :\n",
    "        print(\"Epoch:\" + str(epoch))\n",
    "        print( mean_loss/len(t_data) )\n",
    "        print(\"training acc: \" + str(acc_t / len(t_data)))\n",
    "        print(\"testing acc: \" + str(acc_v / len(verify_data)))\n",
    "\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch:\" + str(epoch))\n",
    "        print( mean_loss/len(t_data) )\n",
    "        print(\"acc: \" + str(acc_t / len(t_data)))\n",
    "        print(\"testing acc: \" + str(acc_v / len(verify_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1350 into shape (16,1,9,10)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-f1b5a45b330a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m  \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m  \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_label\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    297\u001b[0m            [5, 6]])\n\u001b[0;32m    298\u001b[0m     \"\"\"\n\u001b[1;32m--> 299\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reshape'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 1350 into shape (16,1,9,10)"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(1)\n",
    "# class CNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.conv1 = nn.Sequential( # 1, 9, 10\n",
    "#             nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3,4) , stride=1, padding=2, ) , # 16, 9, 9\n",
    "#             nn.MaxPool2d(kernel_size=3 ),  # 16, 3, 3\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "#         self.out = nn.Linear(144, output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         return self.out(x)\n",
    "\n",
    "# cnn = CNN()\n",
    "\n",
    "# criterion = nn.MultiLabelSoftMarginLoss()\n",
    "# optimizer = torch.optim.Adam(cnn.parameters(), lr=0.01)\n",
    "# Epoch = 6400\n",
    "\n",
    "# maxt = 0\n",
    "# maxv = 0\n",
    "# minl = 10000\n",
    "# for epoch in range(Epoch):\n",
    "#     acc_t = 0\n",
    "#     acc_v = 0\n",
    "#     mean_loss = 0\n",
    "#     tmp1, tmp2, tmp3 = maxt, maxv, minl\n",
    "#     for i in range(len(t_data)):\n",
    "#         optimizer.zero_grad()\n",
    "#         inputs = torch.autograd.Variable( torch.FloatTensor(  np.reshape(np.asarray(t_data[i:i+16]), (16,1,9,10)) )  )\n",
    "#         outputs = cnn(inputs)\n",
    "#         loss = criterion( outputs, torch.tensor(t_label[i]) )\n",
    "#         mean_loss += loss.item()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# # if there exist better loss or training accuracy or testing accuracy \n",
    "#     minl = min(minl, mean_loss/len(t_data))\n",
    "#     for i in range (len(outputs)):\n",
    "#         p = Prediction(outputs[i])\n",
    "#         if t_label[i][p] == 1:\n",
    "#             acc_t += 1\n",
    "#     var_in = torch.autograd.Variable( torch.FloatTensor(verify_data))\n",
    "#     var_out = cnn(var_in)\n",
    "#     for i in range (len(var_out)):\n",
    "#         p = Prediction(var_out[i])\n",
    "#         if verify_label[i][p] == 1:\n",
    "#             acc_v += 1\n",
    "#     maxt = max(maxt, acc_t / len(t_data))\n",
    "#     maxv = max(maxv, acc_v / len(verify_data))\n",
    "#     if tmp1 != maxt or tmp2 != maxv or tmp3 != minl :\n",
    "#         print(\"Epoch:\" + str(epoch))\n",
    "#         print( mean_loss/len(t_data) )\n",
    "#         print(\"training acc: \" + str(acc_t / len(t_data)))\n",
    "#         print(\"testing acc: \" + str(acc_v / len(verify_data)))\n",
    "\n",
    "\n",
    "#     if epoch % 100 == 0:\n",
    "#         print(\"Epoch:\" + str(epoch))\n",
    "#         print( mean_loss/len(t_data) )\n",
    "#         print(\"acc: \" + str(acc_t / len(t_data)))\n",
    "#         print(\"testing acc: \" + str(acc_v / len(verify_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}