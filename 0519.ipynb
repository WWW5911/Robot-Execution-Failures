{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd063fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d",
   "display_name": "Python 3.9.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import math\n",
    "import time \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "# Data loading\n",
    "df = pd.read_csv('dataset\\\\LP1\\\\LP1_DP.csv')\n",
    "\n",
    "ylabels = df.Class.unique()\n",
    "output_dim = len(ylabels)\n",
    "full_data = df.to_numpy()\n",
    "full_data = np.transpose(np.transpose(full_data)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "lp_tmp = []\n",
    "np.random.shuffle(full_data)\n",
    "for tp in full_data:\n",
    "    train_data.append(tp[1:])\n",
    "    lp_tmp.append(tp[0])\n",
    "\n",
    "# oneHot Encoding\n",
    "train_label = np.zeros( ( len(train_data), output_dim ) )\n",
    "for i in range(len(lp_tmp)):\n",
    "    train_label[i][lp_tmp[i]-1] = 1     # let 1-5 turns to 0-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(88, 90)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "#data preprocessing\n",
    "x_scale = preprocessing.scale(train_data)\n",
    "x_normalized = preprocessing.normalize(x_scale, norm='l2')\n",
    "pca=PCA(whiten=True)\n",
    "# newData=pca.fit_transform(x_normalized)\n",
    "newData = x_normalized\n",
    "np.shape(newData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "D\n"
     ]
    }
   ],
   "source": [
    "def initialize(Layer):\n",
    "    Weights = []\n",
    "    Bias = []\n",
    "    for i in range ( 1, len(Layer) ):\n",
    "        weight = np.random.randn( Layer[i-1], Layer[i]) * np.sqrt(2 / Layer[i])\n",
    "        b = np.random.randn(Layer[i], 1) * np.sqrt(2 / Layer[i])\n",
    "        Weights.append(weight)\n",
    "        Bias.append(b)\n",
    "\n",
    "    # to make index of layer 1 from 0 to 1\n",
    "    Weights.insert(0, [])\n",
    "    Bias.insert(0, [])\n",
    "    Weights = np.asarray(Weights)\n",
    "    Bias = np.asarray(Bias)\n",
    "\n",
    "    return Weights, Bias\n",
    "\n",
    "def Minmax_scale(w):\n",
    "    return np.array( (w-w.min()) / (w.max()-w.min()) )\n",
    "minmax = 0\n",
    "def Partition(data, label, ratio):\n",
    "    if minmax == 1 :\n",
    "        data = np.transpose(data).astype(np.float64)\n",
    "        for i in range (len(data) ):\n",
    "            data[i] = Minmax_scale(data[i])\n",
    "        data = np.transpose(data)\n",
    "    t_data = data[0:int(len(data)*ratio) ]\n",
    "    t_label = label[0:int(len(label)*ratio) ]\n",
    "    verify_data = data[int(len(data)*ratio):len(data)]\n",
    "    verify_label = label[int(len(data)*ratio):len(data)]\n",
    "    return t_data, t_label, verify_data, verify_label\n",
    "\n",
    "print(\"D\")\n",
    "\n",
    "def Sigmoid( n ):\n",
    "    l = []\n",
    "    for tmp in n:\n",
    "        if np.sum(tmp) >= 0:\n",
    "            l.append( 1 / (1 + math.exp(-tmp) ) )\n",
    "        else:\n",
    "            l.append( math.exp(tmp) / ( 1 + math.exp(tmp) )  )\n",
    "    return l\n",
    "\n",
    "# def sigmoid(data):\n",
    "#   if np.sum(data) >= 0:\n",
    "#     return  1 / ( 1 + math.exp(-np.sum(data)))\n",
    "#   else:\n",
    "#     return math.exp(np.sum(data)) / (1 + math.exp(np.sum(data)) )\n",
    "\n",
    "def Binary_crossEntropy(y, a):\n",
    "    CE = 0\n",
    "    for i in range( len(y) ):\n",
    "        CE += y[i] * math.log(a[i] + 1e-15 ) + (1 - y[i]) * math.log(1 - a[i] + 1e-15)\n",
    "    return CE\n",
    "def FeedForward(data, Weight, bias):\n",
    "    n = np.matmul( data, Weight) + bias.transpose() \n",
    "    return np.asarray( Sigmoid(n[0]) ) \n",
    "\n",
    "def Prediction(a):\n",
    "    index = 0\n",
    "    for i in range( len(a) ):\n",
    "        if a[i] > a[index]:\n",
    "            index = i\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Python39\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.8\n",
    "Layer = [40, 20, 10, 5]\n",
    "\n",
    "Layer.insert(0, len(newData[0]))\n",
    "Layer.append(output_dim)\n",
    "\n",
    "Weights, Bias = initialize(Layer)\n",
    "t_data, t_label, verify_data, verify_label = Partition(newData, train_label, train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 1\n",
      "Loss: 3.4916230740268794\n",
      "Train accuracy: 0.15714285714285714\n",
      "Verify accuracy: 0.3333333333333333\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 11\n",
      "Loss: 2.1652735069953124\n",
      "Train accuracy: 0.4142857142857143\n",
      "Verify accuracy: 0.2777777777777778\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 21\n",
      "Loss: 2.1609668770740447\n",
      "Train accuracy: 0.4142857142857143\n",
      "Verify accuracy: 0.2777777777777778\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 31\n",
      "Loss: 2.159996827434536\n",
      "Train accuracy: 0.4142857142857143\n",
      "Verify accuracy: 0.2777777777777778\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 41\n",
      "Loss: 2.1590467633168773\n",
      "Train accuracy: 0.4142857142857143\n",
      "Verify accuracy: 0.2777777777777778\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 51\n",
      "Loss: 2.1580313462923595\n",
      "Train accuracy: 0.4142857142857143\n",
      "Verify accuracy: 0.2777777777777778\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 61\n",
      "Loss: 2.1569075968427627\n",
      "Train accuracy: 0.4142857142857143\n",
      "Verify accuracy: 0.2777777777777778\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 71\n",
      "Loss: 2.155624797349606\n",
      "Train accuracy: 0.4142857142857143\n",
      "Verify accuracy: 0.2777777777777778\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 81\n",
      "Loss: 2.1541157555765627\n",
      "Train accuracy: 0.4142857142857143\n",
      "Verify accuracy: 0.2777777777777778\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 91\n",
      "Loss: 2.1522869731610332\n",
      "Train accuracy: 0.4142857142857143\n",
      "Verify accuracy: 0.2777777777777778\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 101\n",
      "Loss: 2.150002946502856\n",
      "Train accuracy: 0.4142857142857143\n",
      "Verify accuracy: 0.2777777777777778\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 111\n",
      "Loss: 2.1470594373512766\n",
      "Train accuracy: 0.4142857142857143\n",
      "Verify accuracy: 0.2777777777777778\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 121\n",
      "Loss: 2.143135732057396\n",
      "Train accuracy: 0.4142857142857143\n",
      "Verify accuracy: 0.2777777777777778\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 131\n",
      "Loss: 2.1377049560693075\n",
      "Train accuracy: 0.4142857142857143\n",
      "Verify accuracy: 0.2777777777777778\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 141\n",
      "Loss: 2.1298560808355926\n",
      "Train accuracy: 0.4142857142857143\n",
      "Verify accuracy: 0.2777777777777778\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 151\n",
      "Loss: 2.11792002594662\n",
      "Train accuracy: 0.4142857142857143\n",
      "Verify accuracy: 0.2777777777777778\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 161\n",
      "Loss: 2.098646400758156\n",
      "Train accuracy: 0.4142857142857143\n",
      "Verify accuracy: 0.2777777777777778\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 171\n",
      "Loss: 2.0653992180756005\n",
      "Train accuracy: 0.4142857142857143\n",
      "Verify accuracy: 0.2777777777777778\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 181\n",
      "Loss: 2.005046299135612\n",
      "Train accuracy: 0.4142857142857143\n",
      "Verify accuracy: 0.2777777777777778\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 191\n",
      "Loss: 1.899450067926077\n",
      "Train accuracy: 0.4857142857142857\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 201\n",
      "Loss: 1.7537986593309052\n",
      "Train accuracy: 0.6428571428571429\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 211\n",
      "Loss: 1.6147452970781317\n",
      "Train accuracy: 0.6428571428571429\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 221\n",
      "Loss: 1.5096081201434102\n",
      "Train accuracy: 0.6428571428571429\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 231\n",
      "Loss: 1.435081300129402\n",
      "Train accuracy: 0.6428571428571429\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 241\n",
      "Loss: 1.3824290809328579\n",
      "Train accuracy: 0.6428571428571429\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 251\n",
      "Loss: 1.3439894668928876\n",
      "Train accuracy: 0.6428571428571429\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 261\n",
      "Loss: 1.314000993716556\n",
      "Train accuracy: 0.6571428571428571\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 271\n",
      "Loss: 1.2894605813801199\n",
      "Train accuracy: 0.6571428571428571\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 281\n",
      "Loss: 1.270044421236559\n",
      "Train accuracy: 0.6571428571428571\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 291\n",
      "Loss: 1.2554393065139733\n",
      "Train accuracy: 0.6714285714285714\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 301\n",
      "Loss: 1.2444046338434431\n",
      "Train accuracy: 0.6857142857142857\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 311\n",
      "Loss: 1.235730396133091\n",
      "Train accuracy: 0.6857142857142857\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 321\n",
      "Loss: 1.228546167289164\n",
      "Train accuracy: 0.6857142857142857\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 331\n",
      "Loss: 1.2222134595675673\n",
      "Train accuracy: 0.6857142857142857\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 341\n",
      "Loss: 1.216163049274662\n",
      "Train accuracy: 0.6857142857142857\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 351\n",
      "Loss: 1.2096746074602003\n",
      "Train accuracy: 0.6714285714285714\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 361\n",
      "Loss: 1.2014406092164354\n",
      "Train accuracy: 0.6714285714285714\n",
      "Verify accuracy: 0.5\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 371\n",
      "Loss: 1.1891927230649857\n",
      "Train accuracy: 0.6714285714285714\n",
      "Verify accuracy: 0.4444444444444444\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 381\n",
      "Loss: 1.1734232860842193\n",
      "Train accuracy: 0.6428571428571429\n",
      "Verify accuracy: 0.4444444444444444\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 391\n",
      "Loss: 1.1601020302212284\n",
      "Train accuracy: 0.6428571428571429\n",
      "Verify accuracy: 0.4444444444444444\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 401\n",
      "Loss: 1.1504530958713775\n",
      "Train accuracy: 0.6428571428571429\n",
      "Verify accuracy: 0.5\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 411\n",
      "Loss: 1.1428400913989416\n",
      "Train accuracy: 0.6571428571428571\n",
      "Verify accuracy: 0.5\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 421\n",
      "Loss: 1.136185467753344\n",
      "Train accuracy: 0.6714285714285714\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 431\n",
      "Loss: 1.1298951257150724\n",
      "Train accuracy: 0.6714285714285714\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 441\n",
      "Loss: 1.1235754494588963\n",
      "Train accuracy: 0.6714285714285714\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 451\n",
      "Loss: 1.116924129210029\n",
      "Train accuracy: 0.6714285714285714\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 461\n",
      "Loss: 1.1097015172932843\n",
      "Train accuracy: 0.7\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 471\n",
      "Loss: 1.1017299402127156\n",
      "Train accuracy: 0.7\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 481\n",
      "Loss: 1.0928950705289187\n",
      "Train accuracy: 0.7142857142857143\n",
      "Verify accuracy: 0.5555555555555556\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 491\n",
      "Loss: 1.08313929766095\n",
      "Train accuracy: 0.7285714285714285\n",
      "Verify accuracy: 0.5555555555555556\n",
      "< Reach maximum Epoch >\n",
      "<<< Status  >>>\n",
      "Number of train_data: 70\n",
      "Number of verify_data: 18\n",
      "Number of Hidden Layer: 4\n",
      "Number of Neuron in each Hidden Layer: [40, 20, 10, 5]\n",
      "Learning Rate: 0.01\n",
      "Epoch: 501\n",
      "Loss: 1.0724512393517733\n",
      "Train accuracy: 0.7428571428571429\n",
      "Verify accuracy: 0.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "Epoch = 501\n",
    "tau = 0.001\n",
    "\n",
    "end_flag = False\n",
    "last_acc_T = 0\n",
    "last_acc_V = 0\n",
    "ce = []\n",
    "writee = False\n",
    "strr = \"\"\n",
    "for ep in range(Epoch):\n",
    "    totalLoss = 0\n",
    "    L = len(Layer)-1\n",
    "    acc_t = 0\n",
    "    acc_v = 0\n",
    "    flag = (ep % 10 == 0)\n",
    "    string = \"\"\n",
    "    for i in range(len(t_data)):\n",
    "        a = []\n",
    "        a.append( t_data[i] )\n",
    "        for l in range(1, L+1 ):\n",
    "            a.append(FeedForward(a[l-1],  Weights[l], Bias[l]) )\n",
    "        \n",
    "        # BackWard\n",
    "        Delta = [ a[L] - t_label[i] ]\n",
    "        for l in range(L-1, 0, -1):\n",
    "            Delta.insert(0, np.matmul( Delta[0], Weights[l+1].transpose() ) * ( a[l]*(1-a[l])  )  )\n",
    "        Delta.insert(0, [])\n",
    "        \n",
    "        # update Weights and Bias\n",
    "        for l in range(1, L+1):\n",
    "            t = np.matmul( Delta[l][np.newaxis].T, a[l-1][np.newaxis] )\n",
    "            Weights[l] = Weights[l] - learning_rate * t.T\n",
    "            Bias[l] = Bias[l] - learning_rate * Delta[l]\n",
    "        \n",
    "        # Calculate loss and training accuracy\n",
    "        if flag : \n",
    "            totalLoss -= Binary_crossEntropy( t_label[i], a[L] )\n",
    "            p = Prediction(a[L])\n",
    "            if t_label[i][p] == 1:\n",
    "                acc_t += 1\n",
    "\n",
    "    # Calculate verify accuracy\n",
    "    if flag : \n",
    "        for i in range( len(verify_data) ):\n",
    "            out = verify_data[i]\n",
    "            for l in range(1, L+1 ):\n",
    "                out = FeedForward(out, Weights[l], Bias[l])\n",
    "            p = Prediction(out)\n",
    "            if verify_label[i][p] == 1:\n",
    "                acc_v += 1\n",
    "\n",
    "        if last_acc_T < acc_t / len(t_data) and last_acc_V > acc_v / len(verify_data):\n",
    "            print(\"< Overtraining leads to overfitting >\" )\n",
    "            end_flag = True\n",
    "        last_acc_T = acc_t\n",
    "        last_acc_V = acc_v\n",
    "\n",
    "    if (ep+1) == Epoch:\n",
    "        print(\"< Reach maximum Epoch >\" )\n",
    "        end_flag = True\n",
    "\n",
    "    if flag and totalLoss / len(t_data) < tau:\n",
    "        print(\"< Loss is low enough ( tau: \" + str(tau) + \") >\"  )\n",
    "        end_flag = True\n",
    "\n",
    "    if flag:\n",
    "        print(\"<<< Status\" + \" \" + strr + \" >>>\")\n",
    "        print(\"Number of train_data: \" + str( len(t_data)) )\n",
    "        print(\"Number of verify_data: \" + str( len(verify_data)) )\n",
    "        print(\"Number of Hidden Layer: \" + str( len(Layer)-2) )\n",
    "        print(\"Number of Neuron in each Hidden Layer: \" + str(Layer[1:len(Layer)-1]) )\n",
    "        print(\"Learning Rate: \" + str(learning_rate) )\n",
    "        print(\"Epoch: \" + str(ep+1) )\n",
    "        print(\"Loss: \" + str( totalLoss / len(t_data) ))\n",
    "        print(\"Train accuracy: \" + str(acc_t / len(t_data)) )\n",
    "        print(\"Verify accuracy: \" + str(acc_v / len(verify_data)) )\n",
    "        \n",
    "    if writee :    \n",
    "        with open('Results\\\\' + strr + ' ' + 'Result '+time.strftime('%Y-%m-%d %H-%M-%S', time.localtime()) +  '.txt', mode = 'w') as text_file:\n",
    "            string += \"<<<Status>>>\\n\"\n",
    "            string += \"Number of train_data: \" + str( len(t_data)) +\"\\n\"\n",
    "            string += \"Number of verify_data: \" + str( len(verify_data)) +\"\\n\"\n",
    "            string += \"Number of Hidden Layer: \" + str( len(Layer)-2) +\"\\n\"\n",
    "            string += \"Number of Neuron in each Hidden Layer: \" + str(Layer[1:len(Layer)-1]) +\"\\n\"\n",
    "            string += \"Learning Rate: \" + str(learning_rate) +\"\\n\"\n",
    "            string += \"Epoch: \" + str(ep+1) +\"\\n\"\n",
    "            string += \"Loss: \" + str( totalLoss / len(t_data) )+\"\\n\"\n",
    "            string += \"Train accuracy: \" + str(acc_t / len(t_data)) +\"\\n\"\n",
    "            string += \"Verify accuracy: \" + str(acc_v / len(verify_data)) +\"\\n\"\n",
    "            text_file.write(string)\n",
    "            text_file.close()\n",
    "\n",
    "    if end_flag:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:0\n",
      "0.010909966552744108\n",
      "training acc: 0.2\n",
      "testing acc: 0.2777777777777778\n",
      "Epoch:1\n",
      "0.010731913070289456\n",
      "training acc: 0.5857142857142857\n",
      "testing acc: 0.2777777777777778\n",
      "Epoch:2\n",
      "0.010477960409254444\n",
      "training acc: 0.5571428571428572\n",
      "testing acc: 0.2777777777777778\n",
      "Epoch:3\n",
      "0.010134855505763268\n",
      "training acc: 0.5428571428571428\n",
      "testing acc: 0.2777777777777778\n",
      "Epoch:4\n",
      "0.009835112960058817\n",
      "training acc: 0.5285714285714286\n",
      "testing acc: 0.5555555555555556\n",
      "Epoch:5\n",
      "0.009614541091176928\n",
      "training acc: 0.7142857142857143\n",
      "testing acc: 0.5555555555555556\n",
      "Epoch:6\n",
      "0.009540217211660073\n",
      "training acc: 0.7\n",
      "testing acc: 0.5555555555555556\n",
      "Epoch:7\n",
      "0.00950602626161916\n",
      "training acc: 0.7\n",
      "testing acc: 0.5555555555555556\n",
      "Epoch:8\n",
      "0.00943023688939153\n",
      "training acc: 0.7428571428571429\n",
      "testing acc: 0.5555555555555556\n",
      "Epoch:9\n",
      "0.00934252095161652\n",
      "training acc: 0.7714285714285715\n",
      "testing acc: 0.5\n",
      "Epoch:10\n",
      "0.009282838337275447\n",
      "training acc: 0.8\n",
      "testing acc: 0.5\n",
      "Epoch:11\n",
      "0.009213058635288355\n",
      "training acc: 0.8142857142857143\n",
      "testing acc: 0.5555555555555556\n",
      "Epoch:12\n",
      "0.009137904952679363\n",
      "training acc: 0.8285714285714286\n",
      "testing acc: 0.6111111111111112\n",
      "Epoch:13\n",
      "0.009136656899841465\n",
      "training acc: 0.8428571428571429\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:14\n",
      "0.009145769866145388\n",
      "training acc: 0.8714285714285714\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:15\n",
      "0.009033932123257189\n",
      "training acc: 0.8571428571428571\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:16\n",
      "0.009017564176904911\n",
      "training acc: 0.8714285714285714\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:17\n",
      "0.008999054334601578\n",
      "training acc: 0.8714285714285714\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:18\n",
      "0.008970472777680474\n",
      "training acc: 0.8857142857142857\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:19\n",
      "0.008881713953249309\n",
      "training acc: 0.9285714285714286\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:21\n",
      "0.008857827007162327\n",
      "training acc: 0.9285714285714286\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:25\n",
      "0.008812322663713474\n",
      "training acc: 0.9285714285714286\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:26\n",
      "0.00880854561164671\n",
      "training acc: 0.9285714285714286\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:30\n",
      "0.008801168542431325\n",
      "training acc: 0.9285714285714286\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:35\n",
      "0.008800128223944685\n",
      "training acc: 0.9285714285714286\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:36\n",
      "0.008798025609279165\n",
      "training acc: 0.9285714285714286\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:37\n",
      "0.008797949928106093\n",
      "training acc: 0.9285714285714286\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:39\n",
      "0.008790552432135659\n",
      "training acc: 0.9285714285714286\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:40\n",
      "0.008753760476805726\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:41\n",
      "0.008750435085016856\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:44\n",
      "0.008746701302577037\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:47\n",
      "0.008745986305329264\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:48\n",
      "0.008746444557090196\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7777777777777778\n",
      "Epoch:50\n",
      "0.008745710322139213\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7777777777777778\n",
      "Epoch:51\n",
      "0.008745110345130064\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7777777777777778\n",
      "Epoch:52\n",
      "0.00874473991898858\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7777777777777778\n",
      "Epoch:53\n",
      "0.00874411264700549\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7777777777777778\n",
      "Epoch:54\n",
      "0.008743899823451529\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7777777777777778\n",
      "Epoch:58\n",
      "0.008743810817903402\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:63\n",
      "0.008743742541695127\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:66\n",
      "0.00874368594921365\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:70\n",
      "0.008743634435291194\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:71\n",
      "0.008743628865602066\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:73\n",
      "0.00874361366489712\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:74\n",
      "0.008743526906687386\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:75\n",
      "0.008743493850437961\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:77\n",
      "0.008743418033938018\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:78\n",
      "0.008743286546395751\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:79\n",
      "0.008743164980289887\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:80\n",
      "0.008742812624087139\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:81\n",
      "0.008741689804865388\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:82\n",
      "0.008740179912776362\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:88\n",
      "0.008740095246811302\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:91\n",
      "0.008740039323361553\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:93\n",
      "0.00873991917894811\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:95\n",
      "0.008739772063432908\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:96\n",
      "0.00873974570814444\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:98\n",
      "0.008739437333175113\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:103\n",
      "0.008739290474628915\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.6111111111111112\n",
      "Epoch:105\n",
      "0.008739124953138585\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.6111111111111112\n",
      "Epoch:106\n",
      "0.008738968387550236\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.5555555555555556\n",
      "Epoch:107\n",
      "0.008738829818915347\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.5\n",
      "Epoch:108\n",
      "0.008738564488535024\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.4444444444444444\n",
      "Epoch:109\n",
      "0.008738474602601966\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.4444444444444444\n",
      "Epoch:110\n",
      "0.008738097729427474\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.5\n",
      "Epoch:112\n",
      "0.00873760993687474\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.5\n",
      "Epoch:113\n",
      "0.00873739637738588\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.6111111111111112\n",
      "Epoch:114\n",
      "0.008737273284671257\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.6111111111111112\n",
      "Epoch:115\n",
      "0.008736983391703392\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.6111111111111112\n",
      "Epoch:117\n",
      "0.008736845762753973\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:118\n",
      "0.00873679120777821\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:120\n",
      "0.00873673073339219\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:123\n",
      "0.008736685785103818\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:124\n",
      "0.008736679654340355\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:126\n",
      "0.008736620688316774\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:129\n",
      "0.008736609480514818\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:130\n",
      "0.008736598011182279\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:132\n",
      "0.008736595497751722\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:133\n",
      "0.008736577439977198\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:136\n",
      "0.008736576991421836\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:137\n",
      "0.008736574902218214\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:139\n",
      "0.00873657395949169\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:140\n",
      "0.008736564165779523\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:143\n",
      "0.008736563814537865\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:147\n",
      "0.008736561132328851\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:151\n",
      "0.008736556026400352\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:155\n",
      "0.008736555205315959\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:156\n",
      "0.008736554102934137\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:159\n",
      "0.008736552884992288\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:160\n",
      "0.008736551508915667\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:163\n",
      "0.008736551157674011\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:164\n",
      "0.00873654948661522\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:165\n",
      "0.008736548610791867\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:168\n",
      "0.008736547883980128\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:169\n",
      "0.00873654669796934\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:170\n",
      "0.008736545787173875\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:171\n",
      "0.008736545700503855\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:172\n",
      "0.008736545604710676\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:173\n",
      "0.008736544937199475\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:174\n",
      "0.008736544027924537\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:175\n",
      "0.008736543021335893\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:176\n",
      "0.008736542417686814\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:177\n",
      "0.008736542119663588\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:178\n",
      "0.008736541698477707\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:179\n",
      "0.008736541076582305\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:180\n",
      "0.008736540255497912\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:181\n",
      "0.008736539334058761\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:182\n",
      "0.008736538601164915\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:183\n",
      "0.008736537938215294\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:184\n",
      "0.008736537348250953\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:185\n",
      "0.008736536705068179\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:186\n",
      "0.008736535935681694\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:187\n",
      "0.008736535090268874\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:188\n",
      "0.00873653415210393\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:189\n",
      "0.008736533197213191\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:190\n",
      "0.008736532252966141\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:191\n",
      "0.008736531209884857\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:192\n",
      "0.008736530229145167\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:193\n",
      "0.008736529178461249\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:194\n",
      "0.008736528091284694\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:195\n",
      "0.008736526906794431\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:196\n",
      "0.008736525577854138\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:197\n",
      "0.008736524192654357\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:198\n",
      "0.008736522649319805\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:199\n",
      "0.008736520972178908\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:200\n",
      "0.008736519149067451\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:201\n",
      "0.008736517183026489\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:202\n",
      "0.008736514996509162\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:203\n",
      "0.00873651258039231\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:204\n",
      "0.008736509907306457\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:205\n",
      "0.008736506893622632\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:206\n",
      "0.008736503577354003\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:207\n",
      "0.00873649976387316\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:208\n",
      "0.008736495465344312\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:209\n",
      "0.008736490538837958\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:210\n",
      "0.008736484903766184\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:211\n",
      "0.008736478423281593\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:212\n",
      "0.008736470987906262\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:213\n",
      "0.008736462570270713\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:214\n",
      "0.008736453073061243\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:215\n",
      "0.008736442882491618\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:216\n",
      "0.008736431924056034\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:217\n",
      "0.008736421065975209\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:218\n",
      "0.008736405723861286\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:219\n",
      "0.008736379085754862\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:220\n",
      "0.008736295832359062\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.7222222222222222\n",
      "Epoch:221\n",
      "0.008736088628671607\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:222\n",
      "0.008735452187912805\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:223\n",
      "0.00873268956128432\n",
      "training acc: 0.9428571428571428\n",
      "testing acc: 0.5\n",
      "Epoch:224\n",
      "0.00870653864832557\n",
      "training acc: 0.9571428571428572\n",
      "testing acc: 0.5\n",
      "Epoch:249\n",
      "0.008700645327263948\n",
      "training acc: 0.9571428571428572\n",
      "testing acc: 0.5555555555555556\n",
      "Epoch:3209\n",
      "0.008794573597153839\n",
      "training acc: 0.9285714285714286\n",
      "testing acc: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input, output):\n",
    "        super(MLP,self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Linear(16, 8),\n",
    "        )\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Linear(8, output),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        return x\n",
    "model = MLP(len(t_data[0]), output_dim)\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "Epoch = 6400\n",
    "\n",
    "maxt = 0\n",
    "maxv = 0\n",
    "minl = 10000\n",
    "for epoch in range(Epoch):\n",
    "    acc_t = 0\n",
    "    acc_v = 0\n",
    "    mean_loss = 0\n",
    "    tmp1, tmp2, tmp3 = maxt, maxv, minl\n",
    "    optimizer.zero_grad()\n",
    "    inputs = torch.autograd.Variable( torch.FloatTensor(t_data))\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion( outputs, torch.tensor(t_label) )\n",
    "    mean_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    minl = min(minl, mean_loss/len(t_data))\n",
    "    for i in range (len(outputs)):\n",
    "        p = Prediction(outputs[i])\n",
    "        if t_label[i][p] == 1:\n",
    "            acc_t += 1\n",
    "    var_in = torch.autograd.Variable( torch.FloatTensor(verify_data))\n",
    "    var_out = model(var_in)\n",
    "    for i in range (len(var_out)):\n",
    "        p = Prediction(var_out[i])\n",
    "        if verify_label[i][p] == 1:\n",
    "            acc_v += 1\n",
    "    maxt = max(maxt, acc_t / len(t_data))\n",
    "    maxv = max(maxv, acc_v / len(verify_data))\n",
    "    if tmp1 != maxt or tmp2 != maxv or tmp3 != minl :\n",
    "        print(\"Epoch:\" + str(epoch))\n",
    "        print( mean_loss/len(t_data) )\n",
    "        print(\"training acc: \" + str(acc_t / len(t_data)))\n",
    "        print(\"testing acc: \" + str(acc_v / len(verify_data)))\n",
    "\n",
    "    # if epoch % 100 == 0:\n",
    "    #     print(\"Epoch:\" + str(epoch))\n",
    "    #     print( mean_loss/len(t_data) )\n",
    "    #     for i in range (len(outputs)):\n",
    "    #         p = Prediction(outputs[i])\n",
    "    #         if t_label[i][p] == 1:\n",
    "    #             acc_t += 1\n",
    "    #     print(\"acc: \" + str(acc_t / len(t_data)))\n",
    "\n",
    "    #     var_in = torch.autograd.Variable( torch.FloatTensor(verify_data))\n",
    "    #     var_out = model(var_in)\n",
    "    #     for i in range (len(var_out)):\n",
    "    #         p = Prediction(var_out[i])\n",
    "    #         if verify_label[i][p] == 1:\n",
    "    #             acc_v += 1\n",
    "    #     print(\"testing acc: \" + str(acc_v / len(verify_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.6017, dtype=torch.float64, grad_fn=<MeanBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}