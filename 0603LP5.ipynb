{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd063fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d",
   "display_name": "Python 3.9.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import math\n",
    "import time \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "np.random.seed(4545444) # 4545444\n",
    "\n",
    "# Data loading\n",
    "df = pd.read_csv('dataset\\\\LP5\\\\LP5_DP.csv')\n",
    "\n",
    "ylabels = df.Class.unique()\n",
    "output_dim = len(ylabels)\n",
    "full_data = df.to_numpy()\n",
    "full_data = np.transpose(np.transpose(full_data)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "lp_tmp = []\n",
    "np.random.shuffle(full_data)\n",
    "for tp in full_data:\n",
    "    train_data.append(tp[1:])\n",
    "    lp_tmp.append(tp[0])\n",
    "\n",
    "# oneHot Encoding\n",
    "train_label = np.zeros( ( len(train_data), output_dim ) )\n",
    "for i in range(len(lp_tmp)):\n",
    "    train_label[i][lp_tmp[i]-1] = 1     # let 1-5 turns to 0-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(164, 90)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "#data preprocessing\n",
    "x_scale = preprocessing.scale(train_data)\n",
    "x_normalized = ( preprocessing.normalize(x_scale, norm='l2')  ) \n",
    "# if x_normalized.min() < 0:\n",
    "#     x_normalized = x_normalized - x_normalized.min()\n",
    "#     x_normalized = x_normalized * 255 / x_normalized.max()\n",
    "pca=PCA(whiten=True)\n",
    "#newData=pca.fit_transform(x_normalized)\n",
    "newData = x_normalized\n",
    "np.shape(newData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Minmax_scale(w):\n",
    "    return np.array( (w-w.min()) / (w.max()-w.min()) )\n",
    "minmax = 0\n",
    "def Partition(data, label, ratio):\n",
    "    if minmax == 1 :\n",
    "        data = np.transpose(data).astype(np.float64)\n",
    "        for i in range (len(data) ):\n",
    "            data[i] = Minmax_scale(data[i])\n",
    "        data = np.transpose(data)\n",
    "    t_data = data[0:int(len(data)*ratio) ]\n",
    "    t_label = label[0:int(len(label)*ratio) ]\n",
    "    verify_data = data[int(len(data)*ratio):len(data)]\n",
    "    verify_label = label[int(len(data)*ratio):len(data)]\n",
    "    return t_data, t_label, verify_data, verify_label\n",
    "\n",
    "def Prediction(a):\n",
    "    index = 0\n",
    "    for i in range( len(a) ):\n",
    "        if a[i] > a[index]:\n",
    "            index = i\n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "train_ratio = 0.8\n",
    "t_data, t_label, verify_data, verify_label = Partition(newData, train_label, train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "16.0, 34.0, 25.0, 35.0, 21.0\n5.0, 10.0, 4.0, 9.0, 5.0\n"
     ]
    }
   ],
   "source": [
    "print(str(sum(t_label.T[0])) + \", \" + str(sum(t_label.T[1])) + \", \" + str(sum(t_label.T[2]))+ \", \" + str(sum(t_label.T[3]))+ \", \" + str(sum(t_label.T[4])) )\n",
    "print(str(sum(verify_label.T[0])) + \", \" + str(sum(verify_label.T[1])) + \", \" + str(sum(verify_label.T[2]))+ \", \" + str(sum(verify_label.T[3]))+ \", \" + str(sum(verify_label.T[4])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(131, 144)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "#cnn CONV\n",
    "np.random.seed(121)\n",
    "def create_filter(in_ch, out_ch, keneral):\n",
    "    return np.random.randint(5, size=(out_ch, keneral[0], keneral[1] )) -2\n",
    "\n",
    "def conv(data, l1_filter , stride, padding ):\n",
    "    if padding > 0:\n",
    "        for i in range(padding):\n",
    "            data = np.insert(data, 0, np.zeros( len(data) ), axis=1 )\n",
    "            data = np.insert(data, len(data[0]), np.zeros( len(data) ), axis=1 )\n",
    "            data = np.insert(data, 0, np.zeros( len(data[0]) ), axis=0 )\n",
    "            data = np.insert(data, len(data), np.zeros( len(data[0]) ), axis=0 )\n",
    "    result = []\n",
    "    keneral = [ len(l1_filter[0]), len(l1_filter[0][0]) ]\n",
    "    for filt in l1_filter:\n",
    "        i = 0\n",
    "        tmp = []\n",
    "        while(i <= len(data)- keneral[0]):\n",
    "            j = 0\n",
    "            while(j <= len(data[i])- keneral[1] ):\n",
    "                tmp.append( sum(sum( data[i : i+keneral[0] ,j : j+keneral[1] ] * filt) )    )\n",
    "                j += stride\n",
    "            i += stride\n",
    "        tmp = np.reshape(tmp, (len(data)- keneral[0]+1, len(data[i])- keneral[1]+1)  )\n",
    "        result.append(tmp)\n",
    "    return result\n",
    "\n",
    "def pooling(data, fuct, size):\n",
    "    result = []\n",
    "    for layer in data:\n",
    "        i = 0\n",
    "        tmp = []\n",
    "        while(i < len(layer) ):\n",
    "            j = 0\n",
    "            while(j < len(layer[i]) ):\n",
    "                tmp.append( fuct([fuct( layer[k][j:j+size] )for k in range(i,i+size)] )    )\n",
    "                j += size\n",
    "            i += size\n",
    "        tmp = np.reshape(tmp, ( int(len(layer)/size), int(len(layer[0])/size) )  )\n",
    "        result.append(tmp)\n",
    "    return result\n",
    "\n",
    "l1_filter = create_filter( 1, 16, [3,4])\n",
    "cnn_t_data = [  np.ndarray.flatten( np.asarray( pooling( conv(  np.reshape(np.asarray(t_data[i]), (9,10)), l1_filter , 1, 1), max, 3 ) ) ) for i in range(len(t_data) )]\n",
    "cnn_verify_data = [  np.ndarray.flatten( np.asarray( pooling( conv(  np.reshape(np.asarray(verify_data[i]), (9,10)), l1_filter , 1, 1), max, 3 ) ) ) for i in range(len(verify_data) )]\n",
    "l2_filter = create_filter( 1, 16, [3,3])\n",
    "cnn_t_data = [  np.ndarray.flatten( np.asarray( pooling( conv(  np.reshape(np.asarray(cnn_t_data[i]), (12,12)), l2_filter , 1, 1), max, 4 ) ) ) for i in range(len(cnn_t_data) )]\n",
    "cnn_verify_data = [  np.ndarray.flatten( np.asarray( pooling( conv(  np.reshape(np.asarray(cnn_verify_data[i]), (12,12)), l2_filter , 1, 1), max, 4 ) ) ) for i in range(len(cnn_verify_data) )]\n",
    "np.shape(cnn_t_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sting acc: 0.696969696969697\n",
      "Epoch:49\n",
      "0.005101382214180704\n",
      "training acc: 0.7099236641221374\n",
      "testing acc: 0.696969696969697\n",
      "Epoch:50\n",
      "0.005098053374828683\n",
      "training acc: 0.7099236641221374\n",
      "testing acc: 0.696969696969697\n",
      "Epoch:51\n",
      "0.0050940357241122895\n",
      "training acc: 0.7251908396946565\n",
      "testing acc: 0.696969696969697\n",
      "Epoch:52\n",
      "0.005089378283995097\n",
      "training acc: 0.7404580152671756\n",
      "testing acc: 0.696969696969697\n",
      "Epoch:53\n",
      "0.0050855063147272825\n",
      "training acc: 0.7633587786259542\n",
      "testing acc: 0.696969696969697\n",
      "Epoch:54\n",
      "0.005079804911118039\n",
      "training acc: 0.7862595419847328\n",
      "testing acc: 0.696969696969697\n",
      "Epoch:55\n",
      "0.005074110078572685\n",
      "training acc: 0.8015267175572519\n",
      "testing acc: 0.696969696969697\n",
      "Epoch:56\n",
      "0.005068924942325582\n",
      "training acc: 0.8015267175572519\n",
      "testing acc: 0.696969696969697\n",
      "Epoch:57\n",
      "0.0050632744529129055\n",
      "training acc: 0.8015267175572519\n",
      "testing acc: 0.696969696969697\n",
      "Epoch:58\n",
      "0.005052201007117022\n",
      "training acc: 0.8244274809160306\n",
      "testing acc: 0.696969696969697\n",
      "Epoch:59\n",
      "0.005041569516511656\n",
      "training acc: 0.8244274809160306\n",
      "testing acc: 0.696969696969697\n",
      "Epoch:60\n",
      "0.005028988910953687\n",
      "training acc: 0.8244274809160306\n",
      "testing acc: 0.696969696969697\n",
      "Epoch:61\n",
      "0.005015664359821763\n",
      "training acc: 0.8244274809160306\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:62\n",
      "0.005000167787439736\n",
      "training acc: 0.8244274809160306\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:63\n",
      "0.004986143224368972\n",
      "training acc: 0.8244274809160306\n",
      "testing acc: 0.6060606060606061\n",
      "Epoch:64\n",
      "0.004972418925123235\n",
      "training acc: 0.8320610687022901\n",
      "testing acc: 0.6363636363636364\n",
      "Epoch:65\n",
      "0.004955829882370301\n",
      "training acc: 0.8320610687022901\n",
      "testing acc: 0.6363636363636364\n",
      "Epoch:66\n",
      "0.004942922870440817\n",
      "training acc: 0.8320610687022901\n",
      "testing acc: 0.6060606060606061\n",
      "Epoch:67\n",
      "0.004931572091641261\n",
      "training acc: 0.8320610687022901\n",
      "testing acc: 0.6060606060606061\n",
      "Epoch:68\n",
      "0.004921446647283379\n",
      "training acc: 0.8549618320610687\n",
      "testing acc: 0.6363636363636364\n",
      "Epoch:69\n",
      "0.004912722106927704\n",
      "training acc: 0.8778625954198473\n",
      "testing acc: 0.6363636363636364\n",
      "Epoch:70\n",
      "0.00490535539726605\n",
      "training acc: 0.8625954198473282\n",
      "testing acc: 0.6363636363636364\n",
      "Epoch:71\n",
      "0.004898325757584117\n",
      "training acc: 0.8931297709923665\n",
      "testing acc: 0.6363636363636364\n",
      "Epoch:72\n",
      "0.004892851096837297\n",
      "training acc: 0.9083969465648855\n",
      "testing acc: 0.696969696969697\n",
      "Epoch:73\n",
      "0.004888460843284246\n",
      "training acc: 0.9236641221374046\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:74\n",
      "0.004884729258046201\n",
      "training acc: 0.9007633587786259\n",
      "testing acc: 0.6363636363636364\n",
      "Epoch:75\n",
      "0.00488055312100716\n",
      "training acc: 0.8778625954198473\n",
      "testing acc: 0.696969696969697\n",
      "Epoch:76\n",
      "0.0048771193061395365\n",
      "training acc: 0.8702290076335878\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:77\n",
      "0.004873303981950168\n",
      "training acc: 0.8854961832061069\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:78\n",
      "0.00486999701303269\n",
      "training acc: 0.9236641221374046\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:79\n",
      "0.0048662679844626135\n",
      "training acc: 0.9236641221374046\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:80\n",
      "0.004862947551098663\n",
      "training acc: 0.9083969465648855\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:81\n",
      "0.004858546744039805\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.6363636363636364\n",
      "Epoch:82\n",
      "0.004854808046256362\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:83\n",
      "0.004850477256680986\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.696969696969697\n",
      "Epoch:84\n",
      "0.004846186661774459\n",
      "training acc: 0.9770992366412213\n",
      "testing acc: 0.6363636363636364\n",
      "Epoch:85\n",
      "0.004842740598796624\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:86\n",
      "0.004836209439556122\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:87\n",
      "0.004831861525758373\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:88\n",
      "0.004825472132905811\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:89\n",
      "0.0048196114614900264\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:90\n",
      "0.004812394130911404\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:91\n",
      "0.004806923507831361\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:92\n",
      "0.004799316280351208\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:93\n",
      "0.004792881145336141\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:94\n",
      "0.004783813641570219\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.696969696969697\n",
      "Epoch:95\n",
      "0.0047768454713272915\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:96\n",
      "0.004768463684944178\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:97\n",
      "0.0047613307747708345\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:98\n",
      "0.0047554249985360736\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:99\n",
      "0.00475084928044711\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:100\n",
      "0.004747261740751363\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:100\n",
      "0.004747261740751363\n",
      "acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:101\n",
      "0.0047442663970454005\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:102\n",
      "0.004742029951999851\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:103\n",
      "0.004740033656954383\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:104\n",
      "0.004738317722554849\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:105\n",
      "0.0047367381619570735\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:106\n",
      "0.004735292512618484\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:107\n",
      "0.00473403272758823\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:108\n",
      "0.0047328343310699865\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:109\n",
      "0.004731688670126765\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:110\n",
      "0.004730706245886281\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:111\n",
      "0.004729875119705938\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:112\n",
      "0.004729148117744745\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:113\n",
      "0.004728516910426385\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:114\n",
      "0.004728023108815849\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:115\n",
      "0.004727601665319884\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:116\n",
      "0.0047272126659091895\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:117\n",
      "0.00472690355846511\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.696969696969697\n",
      "Epoch:118\n",
      "0.004726658442048063\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.696969696969697\n",
      "Epoch:119\n",
      "0.004726409706839606\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:120\n",
      "0.004726189135963298\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:121\n",
      "0.004726017156713431\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.696969696969697\n",
      "Epoch:122\n",
      "0.004725848178709036\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:123\n",
      "0.004725697823639454\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:124\n",
      "0.004725585235428765\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.6666666666666666\n",
      "Epoch:125\n",
      "0.004725472967800133\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.696969696969697\n",
      "Epoch:126\n",
      "0.004725358798213373\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.696969696969697\n",
      "Epoch:127\n",
      "0.004725271196300338\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.696969696969697\n",
      "Epoch:128\n",
      "0.004725181697639064\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:129\n",
      "0.004725087904984298\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:130\n",
      "0.004725008188417398\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:131\n",
      "0.004724924140693105\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:132\n",
      "0.004724837215371457\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:133\n",
      "0.004724767267353211\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:134\n",
      "0.004724695961636949\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:135\n",
      "0.004724626650267579\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:136\n",
      "0.004724567164408356\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:137\n",
      "0.004724508684405733\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:138\n",
      "0.004724450920589682\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:139\n",
      "0.004724399956378144\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:140\n",
      "0.00472434796790172\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:141\n",
      "0.00472430024875863\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:142\n",
      "0.004724255523914488\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:143\n",
      "0.004724210208963176\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:144\n",
      "0.004724170054931489\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:145\n",
      "0.0047241328542142625\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:146\n",
      "0.0047240939588926315\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:147\n",
      "0.00472405846806216\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:148\n",
      "0.004724023881321721\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:149\n",
      "0.004723991453560782\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:150\n",
      "0.004723961704726625\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:151\n",
      "0.004723931560982608\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:152\n",
      "0.004723903490775852\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:153\n",
      "0.0047238769793689645\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:154\n",
      "0.004723851680825081\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:155\n",
      "0.004723828253327297\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:156\n",
      "0.004723804319775276\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:157\n",
      "0.004723781642849354\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:158\n",
      "0.004723759251425549\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:159\n",
      "0.004723737170511343\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:160\n",
      "0.004723717075260875\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:161\n",
      "0.00472369842870788\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:162\n",
      "0.0047236788273552315\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:163\n",
      "0.004723661609354792\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:164\n",
      "0.004723643855430065\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:165\n",
      "0.0047236244548319465\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:166\n",
      "0.004723608386481143\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:167\n",
      "0.00472359171725606\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:168\n",
      "0.004723573244366152\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:169\n",
      "0.004723559034557708\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:170\n",
      "0.004723542650834739\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:171\n",
      "0.004723525953476235\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:172\n",
      "0.0047235129412484395\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:173\n",
      "0.004723497723152112\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:174\n",
      "0.004723483928398446\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:175\n",
      "0.0047234700610536105\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:176\n",
      "0.004723453939909229\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:177\n",
      "0.004723439091020625\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:178\n",
      "0.004723424799243203\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:179\n",
      "0.004723410208417948\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:180\n",
      "0.004723398106184729\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:181\n",
      "0.004723383609832191\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:182\n",
      "0.004723372126881542\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:183\n",
      "0.004723359966644852\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:184\n",
      "0.004723346628277781\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:185\n",
      "0.004723336902450247\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:186\n",
      "0.004723324696713829\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:187\n",
      "0.004723312186719683\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:188\n",
      "0.004723302509865139\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:189\n",
      "0.004723291761856675\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:190\n",
      "0.004723281953712838\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:191\n",
      "0.004723269772636578\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:192\n",
      "0.004723260877613255\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:193\n",
      "0.004723251401860567\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:194\n",
      "0.00472324217375144\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:195\n",
      "0.004723232510095292\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:196\n",
      "0.0047232235008016574\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:197\n",
      "0.004723213836450857\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:198\n",
      "0.004723208502562797\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:199\n",
      "0.004723200013563771\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:200\n",
      "0.004723192058057747\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:200\n",
      "0.004723192058057747\n",
      "acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:201\n",
      "0.004723184420007848\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:202\n",
      "0.004723178064286185\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:203\n",
      "0.004723171299414288\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:204\n",
      "0.004723164234799901\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:205\n",
      "0.004723157829410594\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:206\n",
      "0.0047231512649459\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:207\n",
      "0.0047231467167096475\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:208\n",
      "0.004723138965778741\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:209\n",
      "0.004723132876456253\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:210\n",
      "0.0047231272053144795\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:211\n",
      "0.004723121159407762\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:212\n",
      "0.0047231160762892\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:213\n",
      "0.004723109906734366\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:214\n",
      "0.004723104425927335\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:215\n",
      "0.004723099652276396\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:216\n",
      "0.004723094411819077\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:217\n",
      "0.004723089504794886\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:218\n",
      "0.0047230846780030415\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:219\n",
      "0.004723078924197637\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:220\n",
      "0.004723074171038943\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:221\n",
      "0.00472306828455494\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:222\n",
      "0.004723064553577175\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:223\n",
      "0.004723059032132983\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:224\n",
      "0.004723055339708422\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:225\n",
      "0.004723050367386911\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:226\n",
      "0.004723044525707986\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:227\n",
      "0.004723040003173869\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:228\n",
      "0.00472303896536326\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:229\n",
      "0.004723033600563173\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:230\n",
      "0.0047230285768373875\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:231\n",
      "0.0047230184008751315\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:232\n",
      "0.004723017231427903\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:233\n",
      "0.004723011614816338\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:234\n",
      "0.004723005658172449\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:235\n",
      "0.004723004911073847\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:236\n",
      "0.00472300063375201\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:237\n",
      "0.00472299528006636\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:242\n",
      "0.004722988925039348\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:243\n",
      "0.004722988028590492\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:244\n",
      "0.004722982199067982\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:246\n",
      "0.004722972236016672\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:247\n",
      "0.00472295812589084\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:249\n",
      "0.004722957562527785\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:250\n",
      "0.004722944101207247\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:251\n",
      "0.004722942231203124\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.8181818181818182\n",
      "Epoch:266\n",
      "0.004722927920322763\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:270\n",
      "0.004722896680764666\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:274\n",
      "0.004722879522851655\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:278\n",
      "0.004722850533619842\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7878787878787878\n",
      "Epoch:283\n",
      "0.004722850031733519\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7272727272727273\n",
      "Epoch:284\n",
      "0.004722827846274135\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:285\n",
      "0.004722825669928327\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:295\n",
      "0.0047228234633651415\n",
      "training acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:300\n",
      "0.004722957386086088\n",
      "acc: 0.9923664122137404\n",
      "testing acc: 0.7575757575757576\n",
      "Epoch:311\n",
      "0.0048934365296918175\n",
      "training acc: 0.9236641221374046\n",
      "testing acc: 0.8787878787878788\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(12121212)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input, output):\n",
    "        super(MLP,self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU6()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(16, 8),\n",
    "            \n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(8, 3),\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Linear(3, 8), \n",
    "            nn.ReLU6()\n",
    "        )\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Linear(8, output),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        return x\n",
    "model = MLP(len(cnn_t_data[0]), output_dim)\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.007,weight_decay=0.0001)\n",
    "Epoch = 312\n",
    "\n",
    "maxt = 0\n",
    "maxv = 0\n",
    "minl = 10000\n",
    "for epoch in range(Epoch):\n",
    "    acc_t = 0\n",
    "    acc_v = 0\n",
    "    mean_loss = 0\n",
    "    tmp1, tmp2, tmp3 = maxt, maxv, minl\n",
    "    optimizer.zero_grad()\n",
    "    inputs = torch.autograd.Variable( torch.FloatTensor(cnn_t_data))\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion( outputs, torch.tensor(t_label) )\n",
    "    mean_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# if there exist better loss or training accuracy or testing accuracy \n",
    "    minl = min(minl, mean_loss/len(t_data))\n",
    "    for i in range (len(outputs)):\n",
    "        p = Prediction(outputs[i])\n",
    "        if t_label[i][p] == 1:\n",
    "            acc_t += 1\n",
    "    var_in = torch.autograd.Variable( torch.FloatTensor(cnn_verify_data))\n",
    "    var_out = model(var_in)\n",
    "    for i in range (len(var_out)):\n",
    "        p = Prediction(var_out[i])\n",
    "        if verify_label[i][p] == 1:\n",
    "            acc_v += 1\n",
    "    maxt = max(maxt, acc_t / len(t_data))\n",
    "    maxv = max(maxv, acc_v / len(verify_data))\n",
    "    if tmp1 != maxt or tmp2 != maxv or tmp3 != minl :\n",
    "        print(\"Epoch:\" + str(epoch))\n",
    "        print( mean_loss/len(t_data) )\n",
    "        print(\"training acc: \" + str(acc_t / len(t_data)))\n",
    "        print(\"testing acc: \" + str(acc_v / len(verify_data)))\n",
    "\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch:\" + str(epoch))\n",
    "        print( mean_loss/len(t_data) )\n",
    "        print(\"acc: \" + str(acc_t / len(t_data)))\n",
    "        print(\"testing acc: \" + str(acc_v / len(verify_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(1)\n",
    "# class CNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.conv1 = nn.Sequential( # 1, 9, 10\n",
    "#             nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3,4) , stride=1, padding=2, ) , # 16, 9, 9\n",
    "#             nn.MaxPool2d(kernel_size=3 ),  # 16, 3, 3\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "#         self.out = nn.Linear(144, output_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         return self.out(x)\n",
    "\n",
    "# cnn = CNN()\n",
    "\n",
    "# criterion = nn.MultiLabelSoftMarginLoss()\n",
    "# optimizer = torch.optim.Adam(cnn.parameters(), lr=0.01)\n",
    "# Epoch = 6400\n",
    "\n",
    "# maxt = 0\n",
    "# maxv = 0\n",
    "# minl = 10000\n",
    "# for epoch in range(Epoch):\n",
    "#     acc_t = 0\n",
    "#     acc_v = 0\n",
    "#     mean_loss = 0\n",
    "#     tmp1, tmp2, tmp3 = maxt, maxv, minl\n",
    "#     for i in range(len(t_data)):\n",
    "#         optimizer.zero_grad()\n",
    "#         inputs = torch.autograd.Variable( torch.FloatTensor(  np.reshape(np.asarray(t_data[i:i+16]), (16,1,9,10)) )  )\n",
    "#         outputs = cnn(inputs)\n",
    "#         loss = criterion( outputs, torch.tensor(t_label[i]) )\n",
    "#         mean_loss += loss.item()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# # if there exist better loss or training accuracy or testing accuracy \n",
    "#     minl = min(minl, mean_loss/len(t_data))\n",
    "#     for i in range (len(outputs)):\n",
    "#         p = Prediction(outputs[i])\n",
    "#         if t_label[i][p] == 1:\n",
    "#             acc_t += 1\n",
    "#     var_in = torch.autograd.Variable( torch.FloatTensor(verify_data))\n",
    "#     var_out = cnn(var_in)\n",
    "#     for i in range (len(var_out)):\n",
    "#         p = Prediction(var_out[i])\n",
    "#         if verify_label[i][p] == 1:\n",
    "#             acc_v += 1\n",
    "#     maxt = max(maxt, acc_t / len(t_data))\n",
    "#     maxv = max(maxv, acc_v / len(verify_data))\n",
    "#     if tmp1 != maxt or tmp2 != maxv or tmp3 != minl :\n",
    "#         print(\"Epoch:\" + str(epoch))\n",
    "#         print( mean_loss/len(t_data) )\n",
    "#         print(\"training acc: \" + str(acc_t / len(t_data)))\n",
    "#         print(\"testing acc: \" + str(acc_v / len(verify_data)))\n",
    "\n",
    "\n",
    "#     if epoch % 100 == 0:\n",
    "#         print(\"Epoch:\" + str(epoch))\n",
    "#         print( mean_loss/len(t_data) )\n",
    "#         print(\"acc: \" + str(acc_t / len(t_data)))\n",
    "#         print(\"testing acc: \" + str(acc_v / len(verify_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}